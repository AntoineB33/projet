{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import plotly as plt\n",
    "import plotly.express as px\n",
    "from features import *\n",
    "from clustering import *\n",
    "from utils import *\n",
    "from constant import  PATH_OUTPUT, MODEL_CLUSTERING, PATH_DATA, PATH_DATA_ALL\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from images import load_images_from_folder\n",
    "\n",
    "# Example usage:\n",
    "folder_path = PATH_DATA_ALL + \"/code_test\"\n",
    "images, labels_true, folder_names, smallest_height, smallest_width, smallest_height2, smallest_width2 = load_images_from_folder(folder_path)\n",
    "taille = len(images)\n",
    "nombre_de_canaux = 3\n",
    "# print(f\"Smallest height: {smallest_height}\")\n",
    "# print(f\"Smallest width: {smallest_width}\")\n",
    "# print(f\"Hightest height: {smallest_height2}\")\n",
    "# print(f\"Hightest width: {smallest_width2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Supposons que `images` est votre liste d'images en RGB\n",
    "descriptors_hsv = convert_color_space(images, \"HSV\") # ou \"Lab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors_sift = compute_sift_descriptors(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_to_use = descriptors_hsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "descriptors_hog = compute_hog_descriptors(images_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors_hist = compute_gray_histograms(images_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42, 8192)\n",
      "(42, 256, 256, 3)\n",
      "(42,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\comma\\AppData\\Local\\Temp\\ipykernel_1868\\3989002225.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  print(np.array(descriptors_sift).shape)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(descriptors_hog).shape)\n",
    "print(np.array(descriptors_hsv).shape)\n",
    "print(np.array(descriptors_sift).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Application de RBM et KMEANS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -10.53, time = 0.22s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -4.72, time = 0.07s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -4.03, time = 0.05s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -2.37, time = 0.03s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -2.35, time = 0.02s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -1.78, time = 0.03s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -1.73, time = 0.03s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -0.99, time = 0.04s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -1.57, time = 0.03s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -1.04, time = 0.03s\n",
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -185.16, time = 0.03s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -179.98, time = 0.02s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -177.10, time = 0.02s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -174.68, time = 0.03s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -172.30, time = 0.03s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -171.51, time = 0.04s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -170.87, time = 0.04s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -167.88, time = 0.03s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -170.60, time = 0.03s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -166.23, time = 0.03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\comma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1474: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (20). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "descriptors = descriptors_hist\n",
    "\n",
    "# Présumons que `images` est votre liste d'images prétraitées et aplatie en vecteurs\n",
    "\n",
    "# Initialisation de la classe StackedRBM\n",
    "stacked_rbm = StackedRBM(n_components_list=[256, 128], n_iter=10, learning_rate=0.01, batch_size=10)\n",
    "\n",
    "# Ajustement des RBMs sur les données d'image\n",
    "stacked_rbm.fit(descriptors)\n",
    "\n",
    "# Transformation des images en nouvelles représentations avec les RBMs entraînés\n",
    "transformed_images = stacked_rbm.transform(descriptors)\n",
    "\n",
    "# Normalisation des caractéristiques pour améliorer les performances de K-Means\n",
    "scaler = StandardScaler()\n",
    "transformed_images_scaled = scaler.fit_transform(transformed_images)\n",
    "\n",
    "# Clustering avec K-Means\n",
    "kmeans = KMeans(n_clusters=20, random_state=42)\n",
    "clusters = kmeans.fit_predict(transformed_images_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Calcul du Silhouette Score\n",
    "score = silhouette_score(transformed_images_scaled, clusters)\n",
    "\n",
    "print(\"Silhouette Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
