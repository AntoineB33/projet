{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import plotly as plt\n",
    "import plotly.express as px\n",
    "from features import *\n",
    "from clustering import *\n",
    "from utils import *\n",
    "from constant import  PATH_OUTPUT, MODEL_CLUSTERING, PATH_DATA, PATH_DATA_ALL\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from images import load_images_from_folder\n",
    "\n",
    "# Example usage:\n",
    "folder_path = PATH_DATA_ALL + \"/code_test\"\n",
    "images, labels_true, folder_names, smallest_height, smallest_width, smallest_height2, smallest_width2 = load_images_from_folder(folder_path)\n",
    "taille = len(images)\n",
    "nombre_de_canaux = 3\n",
    "# print(f\"Smallest height: {smallest_height}\")\n",
    "# print(f\"Smallest width: {smallest_width}\")\n",
    "# print(f\"Hightest height: {smallest_height2}\")\n",
    "# print(f\"Hightest width: {smallest_width2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Supposons que `images` est votre liste d'images en RGB\n",
    "descriptors_hsv = convert_color_space(images, \"HSV\") # ou \"Lab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\comma\\Documents\\travail\\Polytech\\s8\\apprentissage_automatique\\projet\\sujet_tp\\sujet_tp\\src\\features.py:127: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  features = np.array(\n"
     ]
    }
   ],
   "source": [
    "# descriptors_sift = compute_sift_descriptors(images)\n",
    "sift_descriptors = extract_sift_features(images)\n",
    "\n",
    "# Étape 2 : Création des vecteurs de caractéristiques avec Bag of Features\n",
    "descriptors_sift = create_bag_of_features(sift_descriptors, n_clusters=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_to_use = descriptors_hsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "descriptors_hog = compute_hog_descriptors(images_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors_hist = compute_gray_histograms(images_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42, 8192)\n",
      "(42, 256, 256, 3)\n",
      "(42, 50)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(descriptors_hog).shape)\n",
    "print(np.array(descriptors_hsv).shape)\n",
    "print(np.array(descriptors_sift).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Application de RBM et KMEANS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -431.62, time = 0.02s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -903.82, time = 0.01s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -452.29, time = 0.01s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -862.97, time = 0.01s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -2819.20, time = 0.01s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -1111.95, time = 0.01s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -4582.75, time = 0.01s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -3296.31, time = 0.00s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -4157.86, time = 0.01s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -4930.79, time = 0.01s\n",
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -184.34, time = 0.02s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -179.38, time = 0.01s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -176.59, time = 0.01s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -175.01, time = 0.01s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -174.05, time = 0.02s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -173.41, time = 0.01s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -172.37, time = 0.02s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -171.32, time = 0.02s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -171.16, time = 0.01s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -170.39, time = 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\comma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1474: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (20). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "descriptors = descriptors_sift\n",
    "\n",
    "# Présumons que `images` est votre liste d'images prétraitées et aplatie en vecteurs\n",
    "\n",
    "# Initialisation de la classe StackedRBM\n",
    "stacked_rbm = StackedRBM(n_components_list=[256, 128], n_iter=10, learning_rate=0.01, batch_size=10)\n",
    "\n",
    "# Ajustement des RBMs sur les données d'image\n",
    "stacked_rbm.fit(descriptors)\n",
    "\n",
    "# Transformation des images en nouvelles représentations avec les RBMs entraînés\n",
    "transformed_images = stacked_rbm.transform(descriptors)\n",
    "\n",
    "# Normalisation des caractéristiques pour améliorer les performances de K-Means\n",
    "scaler = StandardScaler()\n",
    "transformed_images_scaled = scaler.fit_transform(transformed_images)\n",
    "\n",
    "# Clustering avec K-Means\n",
    "kmeans = KMeans(n_clusters=20, random_state=42)\n",
    "clusters = kmeans.fit_predict(transformed_images_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "##### Résultat ######\n",
      "########## Métrique descripteur : HISTOGRAM\n",
      "Adjusted Mutual Information: -0.00973927291987954\n",
      "Silhouette Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n##### Résultat ######\")\n",
    "metric_hist = show_metric(labels_true, clusters, transformed_images_scaled, bool_show=True, name_descriptor=\"HISTOGRAM\", bool_return=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
