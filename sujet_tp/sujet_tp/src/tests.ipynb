{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import plotly as plt\n",
    "import plotly.express as px\n",
    "from features import *\n",
    "from clustering import *\n",
    "from utils import *\n",
    "from constant import  PATH_OUTPUT, MODEL_CLUSTERING, PATH_DATA, PATH_DATA_ALL\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from images import load_images_from_folder\n",
    "\n",
    "# Example usage:\n",
    "folder_path = PATH_DATA_ALL + \"/code_test\"\n",
    "images, labels_true, folder_names, smallest_height, smallest_width, smallest_height2, smallest_width2 = load_images_from_folder(folder_path)\n",
    "taille = len(images)\n",
    "nombre_de_canaux = 3\n",
    "# print(f\"Smallest height: {smallest_height}\")\n",
    "# print(f\"Smallest width: {smallest_width}\")\n",
    "# print(f\"Hightest height: {smallest_height2}\")\n",
    "# print(f\"Hightest width: {smallest_width2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Supposons que `images` est votre liste d'images en RGB\n",
    "descriptors_hsv = convert_color_space(images, \"HSV\") # ou \"Lab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de sift_descriptors: 42\n"
     ]
    }
   ],
   "source": [
    "# descriptors_sift = compute_sift_descriptors(images)\n",
    "sift_descriptors = extract_sift_features(descriptors_hsv)\n",
    "print(f\"Taille de sift_descriptors: {len(sift_descriptors)}\")\n",
    "# Étape 2 : Création des vecteurs de caractéristiques avec Bag of Features\n",
    "descriptors_sift = create_bag_of_features(sift_descriptors, n_clusters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_to_use = descriptors_hsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "descriptors_hog = compute_hog_descriptors(images_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors_hist = compute_gray_histograms(images_to_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Conversion des données de clustering au format requis pour la visualisation avec Streamlit.**\n",
    "\n",
    "**TODO :**\n",
    "- Dans le fichier `utils.py`, implémenter la fonction `conversion_3d` afin de convertir un vecteur de dimension n vers une dimension 3 pour a visualisation.\n",
    "- Lien : https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42, 8192)\n",
      "(42, 256, 256, 3)\n",
      "(42, 20)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(descriptors_hog).shape)\n",
    "print(np.array(descriptors_hsv).shape)\n",
    "print(np.array(descriptors_sift).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Application de RBM et KMEANS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -13.74, time = 0.03s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -6.68, time = 0.02s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -4.24, time = 0.05s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -2.81, time = 0.03s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -3.05, time = 0.04s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -2.18, time = 0.02s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -1.97, time = 0.02s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -1.57, time = 0.02s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -1.23, time = 0.02s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -1.37, time = 0.02s\n",
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -186.17, time = 0.02s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -178.84, time = 0.03s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -175.87, time = 0.01s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -174.95, time = 0.01s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -172.86, time = 0.01s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -170.97, time = 0.01s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -167.70, time = 0.01s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -169.93, time = 0.01s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -166.66, time = 0.03s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -163.07, time = 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\comma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1474: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (20). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "descriptors = descriptors_hist\n",
    "\n",
    "# Présumons que `images` est votre liste d'images prétraitées et aplatie en vecteurs\n",
    "\n",
    "# Initialisation de la classe StackedRBM\n",
    "stacked_rbm = StackedRBM(n_components_list=[256, 128], n_iter=10, learning_rate=0.01, batch_size=10)\n",
    "\n",
    "# Ajustement des RBMs sur les données d'image\n",
    "stacked_rbm.fit(descriptors)\n",
    "\n",
    "# Transformation des images en nouvelles représentations avec les RBMs entraînés\n",
    "transformed_images = stacked_rbm.transform(descriptors)\n",
    "\n",
    "# Normalisation des caractéristiques pour améliorer les performances de K-Means\n",
    "scaler = StandardScaler()\n",
    "transformed_images_scaled = scaler.fit_transform(transformed_images)\n",
    "\n",
    "# Clustering avec K-Means\n",
    "kmeans = KMeans(n_clusters=20, random_state=42)\n",
    "clusters = kmeans.fit_predict(transformed_images_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "##### Résultat ######\n",
      "########## Métrique descripteur : HSV_HISTOGRAM\n",
      "Adjusted Mutual Information: -0.00973927291987954\n",
      "Silhouette Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n##### Résultat ######\")\n",
    "# metric_hist = show_metric(labels_true, clusters, transformed_images_scaled, bool_show=True, name_descriptor=\"HISTOGRAM\", bool_return=True)\n",
    "\n",
    "metric_hsv_hist_rbm = show_metric(labels_true, clusters, transformed_images_scaled, bool_show=True, name_descriptor=\"HSV et HISTOGRAM\", name_model = \"Stacked RBM\", bool_return=True)\n",
    "# metric_hsv_hog_rbm = pd.read_excel(\"output/save_clustering_hsv_hog_rbm_kmeans.xlsx\")\n",
    "# metric_hsv_sift_rbm = pd.read_excel(\"output/save_clustering_hsv_sift_rbm_kmeans.xlsx\")\n",
    "# metric_hist_rbm = pd.read_excel(\"output/save_clustering_hist_rbm_kmeans.xlsx\")\n",
    "# metric_hog_rbm = pd.read_excel(\"output/save_clustering_hog_rbm_kmeans.xlsx\")\n",
    "# metric_sift_rbm = pd.read_excel(\"output/save_clustering_sift_rbm_kmeans.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Conversion des données de clustering au format requis pour la visualisation avec Streamlit.**\n",
    "\n",
    "**TODO :**\n",
    "- Dans le fichier `utils.py`, implémenter la fonction `conversion_3d` afin de convertir un vecteur de dimension n vers une dimension 3 pour a visualisation.\n",
    "- Lien : https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "descriptors_hist_norm = scaler.fit_transform(descriptors_hist)\n",
    "descriptors_hog_norm = scaler.fit_transform(descriptors_hog)\n",
    "\n",
    "#conversion vers un format 3D pour la visualisation\n",
    "x_3d_hist = conversion_3d(descriptors_hist_norm)\n",
    "# x_3d_hog = conversion_3d(descriptors_hog_norm)\n",
    "\n",
    "# création des dataframe pour la sauvegarde des données pour la visualisation\n",
    "df_hist = create_df_to_export(x_3d_hist, labels_true, kmeans.labels_)\n",
    "# df_hog = create_df_to_export(x_3d_hog, labels_true, kmeans.labels_)\n",
    "\n",
    "# Vérifie si le dossier existe déjà\n",
    "if not os.path.exists(PATH_OUTPUT):\n",
    "    # Crée le dossier\n",
    "    os.makedirs(PATH_OUTPUT)\n",
    "\n",
    "# sauvegarde des données\n",
    "df_hist.to_excel(PATH_OUTPUT+\"/save_clustering_hsv_hist_rbm_kmeans.xlsx\")\n",
    "# df_hog.to_excel(PATH_OUTPUT+\"/save_clustering_hog_kmeans.xlsx\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1554332066.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[14], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    df_metric = pd.DataFrame(list_dict)df_metric.to_excel(PATH_OUTPUT+\"/save_metric.xlsx\")\u001b[0m\n\u001b[1;37m                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "list_dict = [metric_hsv_hist_rbm]\n",
    "df_metric = pd.DataFrame(list_dict)\n",
    "df_metric.to_excel(PATH_OUTPUT+\"/save_metric.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
